"""CLI for JDC utilities"""

import click
from jdc_utils.submission import CoreMeasures
from jdc_utils.transforms import read_df, run_transformfile
import jdc_utils.dataforge_ids as ids
import jdc_utils.dataforge_tools as tools
from jdc_utils.utils import copy_file
import os
import pandas as pd
import glob 
from pathlib import Path 
import yaml
import sys 

# overall CLI
@click.group()
def cli():
    """CLI for JDC utilities"""
    pass

@click.command(help=''' 


This function is used to replace (and map) local ids assigned at the time of data collection
with separate ids generated by the DASC and given to hubs.

Replacing local ids with JDC specific ideas reduces the risk of hub and participant identification.

The mapped local to JDC ids are intended to be seen only by hub staff. 

IN DEVELOPMENT: Additionally, this function accepts remote version control history (ie git repo) to save previous versions.
'''
)
@click.option(
    "--file-path",
    "file_paths",
    help="Path to a file with locals/old ids to be replaced. Can specify multiple files if need to replace ids across multiple files",
    multiple=True,
    #required=True,
)
@click.option(
    "--id-file",
    help="Path to where the generated ids (created by the MAARC and distributed to hubs) exist",
    #required=True,
)
@click.option(
    "--map-file",
    help="Path to csv where the id mappings are stored -- this will be generated if file does not exist",
    default=None
)
@click.option("--map-url", help='Git bare repo set up -- ie the "remote url" for sharing mapped ids', default=None)
@click.option("--column", help="Name of column across files specified with old (or local) ids. \
    If none specified, defaults to first level (ie 0) pandas dataframe index", default=None)
@click.option("--config-file", help="A configuration file containing all required replace-id fields", default=None)

#TODO: add other possible params
def replace_ids(file_paths, id_file, map_file,map_url,column,config_file=None):
    replace_ids_dir = os.path.join("tmp", "jdc",'replaced_ids')
    os.makedirs(replace_ids_dir, exist_ok=True)

    #if no config file, then need these params
    if not config_file:
        assert map_file
        assert id_file
        assert file_paths
    for file_path in file_paths:
        #glob.glob allows support for both wildcards (*) and actual file paths
        file_path_with_glob_regexs = glob.glob(file_path) #if not a regex, will just return the filepath within list
        for file_path_glob in file_path_with_glob_regexs:
            df = read_df(file_path_glob)

            if config_file:
                with open(config_file,'r') as f:
                    config = yaml.safe_load(f)
                replace_id_params = config['replace_ids']
                df_new = ids.replace_ids(df, **replace_id_params)
            else:
                df_new = ids.replace_ids(
                    df, id_file=id_file, map_file=map_file, map_url=map_url, column=column
                )

            file_name = Path(file_path_glob).stem
            new_file_dir = os.path.join(replace_ids_dir, file_name+'.csv')
            df_new.to_csv(new_file_dir,index=False)
            click.echo(f"Replaced local with jdc ids in: {os.path.join(os.getcwd(),new_file_dir)}")

@click.command(help=''' 
This function takes a specified date field and shifts them around a specified number of days (ie shift amount).
This shifted amount is selected randomly within an interval of the previous 182 days and next 182 days.
This shift amount is fixed within each individual such that the intervals between dates are retained to provide the 
capability to calculate derived variables such as days from a given visit/timepoint (e.g., days from baseline or 
days from release). 

As with the replace id function, the mappings (i.e., id to the random # of days shifted) is stored in a separate file
to reduce deductive disclosure risk from PII linkage with date variables.

However, by storing this shift amount (rather than randomly shifting at each data update, 
the exact dates can be recovered
simply by subtracting this added random amount to the shifted date.

IN DEVELOPMENT: Additionally, this function accepts remote version control history (ie git repo) to save previous versions.
'''
)
@click.option(
    "--file-path",
    "file_paths",
    help="Path to a file with dates to be shifted. Can specify multiple files if column name is the same across files",
    multiple=True,
    #required=True,
)
@click.option(
    "--map-file",
    help="path to csv where the id mappings are stored -- this will be generated if file does not exist",
    default=None
)
@click.option("--map-url", help='Git bare repo set up -- ie the "remote url" for sharing mapped ids (optional)', default=None)
@click.option("--id-column", help="Name of column across files specified ids.", default=None)
@click.option("--date-column", help="Name of date column(s) to be shifted", default=None,multiple=True)
@click.option("--keep-inputs", help="This flag allows users to keep inputs. That is, both the original non-shifted date columns and the amount shifted by.",is_flag=True)
@click.option("--config-file", help="A configuration file containing all required shift date fields", default=None)

def shift_dates(file_paths,map_file,map_url,id_column,date_column,keep_inputs,config_file):
    shifted_dates_dir = os.path.join("tmp", "jdc",'shifted_dates')
    os.makedirs(shifted_dates_dir, exist_ok=True)

    #if no config file, then need these params
    if not config_file:
        assert map_file
        assert file_paths
        assert date_column
    for file_path in file_paths:
        #glob.glob allows support for both wildcards (*) and actual file paths
        file_path_with_glob_regexs = glob.glob(file_path) #if not a regex, will just return the filepath within list
        for file_path_glob in file_path_with_glob_regexs:
            df = read_df(file_path_glob)

            if config_file:
                with open(config_file,'r') as f:
                    config = yaml.safe_load(f)
                params = config['shift_dates']
                df_new = tools.shift_dates(df, **params)
            else:
                df_new = tools.shift_dates(
                    df, 
                     map_file=map_file,
                      map_url=map_url, 
                      id_col=id_column,
                      date_cols=date_column,
                      keep_inputs=keep_inputs
                )

            file_name = Path(file_path_glob).stem
            new_file_dir = os.path.join(shifted_dates_dir, file_name+'.csv')
            df_new.to_csv(new_file_dir,index=False)
            click.echo(f"Shifting dates and saving to: {os.path.join(os.getcwd(),new_file_dir)}")


@click.command(
    ''' 
    This input file transforms a given dataset with a variety of functions or any function 
    available within pandas that has an "inplace" argument. 

    Most commonly, this will entail renaming variables and values to match the data model
    schema in order to pass validation (see the jdc-utils validate function)

    ''' 
)
@click.option("--transform-file", help="Path to the given transform file")
@click.option(
    "--file-path",
    "file_paths",
    help="Path to the given dataset file. Can specify multiple files if same transform file used across multiple data files.",
    multiple=True,
    required=True,
)
def transform(transform_file, file_paths):

    # read in and run transforms -- right now currently using pandas -- may want to migrate to petl
    # for consistency with validation as it uses petl to read in and type conversions may be different.
    # alternatively, we could use the pandas plugin for frictionless but it is experimental.
    #click.echo("STARTING")
    for file_path in file_paths:
        #glob.glob allows support for both wildcards (*) and actual file paths
        file_path_with_glob_regexs = glob.glob(file_path) #if not a regex, will just return the filepath within list
        
        print(f"Applying {transform_file} to:")
        print(','.join(file_path_with_glob_regexs))

        for file_path_glob in file_path_with_glob_regexs:
            df = read_df(file_path_glob)
            df = run_transformfile(df, transform_file)

            # make transform dir
            transform_dir = os.path.join("tmp","jdc","transformed")
            os.makedirs(transform_dir, exist_ok=True)

            # save file
            file_name = os.path.split(file_path_glob)[-1]
            file_path_to_save = os.path.join(transform_dir, file_name)
            df.to_csv(file_path_to_save,index=False)
            click.echo(f"Transformed file saved to {file_path_to_save}")


@click.command()
@click.option(
    "--filepath",
    help="Path to directory where dataset file(s) live",
)
@click.option(
    "--outdir",
    help="Path to directory where dataset file(s) live",
)
def validate(filepath,outdir='.'):
    os.chdir(filepath)
    core_measures = CoreMeasures('.')
    core_measures.validate(write_to_file=True)

    if core_measures.report["valid"]:
        click.echo(
            f"Congrats! Your file(s) -- {','.join(core_measures.resource_names)} ---  passed validation!\n"
        )
        click.echo("If you're happy with them, you can proceed with submission.")
    else:
        click.echo(
            "\n\n"
            f"One or more of the core measure files need some corrections.\n"
            f"Take a look below at the error report table below to correct.\n"
            f"We also wrote several useful files derived from the validation report to help in\n"
            "the curation process with the suffix 'report':\n"
            "----------------------------------------------------------------------\n"
            "----------------------------------------------------------------------\n"
        )
        click.echo(core_measures.report.to_summary())

cli.add_command(replace_ids, name="replace-ids")
cli.add_command(shift_dates, name="shift-dates")
cli.add_command(transform, name="transform")
cli.add_command(validate, name="validate")


if __name__ == "__main__":
    cli()
